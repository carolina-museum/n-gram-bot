{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848361d8",
   "metadata": {},
   "source": [
    "# n-gram bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab0b89",
   "metadata": {},
   "source": [
    "## A sample project on making n-gram bot that responds the continuing sequence of the given word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78422327",
   "metadata": {},
   "source": [
    "Reference: Coursera course \"Gen AI Foundational Models for NLP & Language Understanding\" by IBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466409d-7e39-4b34-adc7-783c716bcb18",
   "metadata": {},
   "source": [
    "User choices are the amount(number) of context and the starting word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a33f8e",
   "metadata": {},
   "source": [
    "This project was made on Oct 1st, 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cee49c",
   "metadata": {},
   "source": [
    "### Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500ec64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!mamba install -y nltk\n",
    "!pip install torchtext -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d32b39",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f1a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import string\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "%capture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8fe544",
   "metadata": {},
   "source": [
    "### Define a function to preprocess the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53bbe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff8d82",
   "metadata": {},
   "source": [
    "### Input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5e6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"The article is “Historical review and future challenges in Supercomputing and Networks of Scientific Communication”, written by Á. Fernández-González, R. Rosillo, J. Á. Miguel-Dávila, and V. Matellán. It was published in The Journal of Supercomputing in November 2015. The article reviews the history of supercomputers covering their birth, current use, and future challenges. The characteristics, improvements, and challenges of major supercomputers are covered. The authors also mention the importance of networks that support the functions of supercomputers, along with the history of how the networks have been developing. The authors have written an important and timely article on supercomputing and their networks and they take on the difficult task of revealing the historical plots and improvements of supercomputers. Unfortunately, the authors have not covered how they became more energy efficient in detail.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f138048",
   "metadata": {},
   "source": [
    "### Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228c2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def preprocess(words):\n",
    "    tokens=word_tokenize(words)\n",
    "    tokens=[preprocess_string(w)   for w in tokens]\n",
    "    return [w.lower()  for w in tokens if len(w)!=0 or not(w in string.punctuation) ]\n",
    "\n",
    "tokens=preprocess(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c217c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n",
      "['the', 'article', 'is', 'historical', 'review', 'and', 'future', 'challenges', 'in', 'supercomputing', 'and', 'networks', 'of', 'scientific', 'communication', 'written', 'by', 'á', 'fernándezgonzález', 'r', 'rosillo', 'j', 'á', 'migueldávila', 'and', 'v', 'matellán', 'it', 'was', 'published', 'in', 'the', 'journal', 'of', 'supercomputing', 'in', 'november', 'the', 'article', 'reviews', 'the', 'history', 'of', 'supercomputers', 'covering', 'their', 'birth', 'current', 'use', 'and', 'future', 'challenges', 'the', 'characteristics', 'improvements', 'and', 'challenges', 'of', 'major', 'supercomputers', 'are', 'covered', 'the', 'authors', 'also', 'mention', 'the', 'importance', 'of', 'networks', 'that', 'support', 'the', 'functions', 'of', 'supercomputers', 'along', 'with', 'the', 'history', 'of', 'how', 'the', 'networks', 'have', 'been', 'developing', 'the', 'authors', 'have', 'written', 'an', 'important', 'and', 'timely', 'article', 'on', 'supercomputing', 'and', 'their', 'networks', 'and', 'they', 'take', 'on', 'the', 'difficult', 'task', 'of', 'revealing', 'the', 'historical', 'plots', 'and', 'improvements', 'of', 'supercomputers', 'unfortunately', 'the', 'authors', 'have', 'not', 'covered', 'how', 'they', 'became', 'more', 'energy', 'efficient', 'in', 'detail']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e133c32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 72 samples and 131 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(tokens)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b72439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n"
     ]
    }
   ],
   "source": [
    "#total count of each word \n",
    "C=sum(fdist.values())\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e6c21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "{'functions', 'important', 'was', 'matellán', 'r', 'that', 'mention', 'how', 'developing', 'with', 'they', 'unfortunately', 'not', 'efficient', 'the', 'of', 'review', 'it', 'history', 'future', 'task', 'november', 'reviews', 'communication', 'detail', 'more', 'current', 'energy', 'scientific', 'support', 'on', 'an', 'revealing', 'á', 'been', 'have', 'and', 'birth', 'migueldávila', 'by', 'covered', 'v', 'published', 'plots', 'fernándezgonzález', 'characteristics', 'networks', 'use', 'in', 'authors', 'challenges', 'major', 'also', 'timely', 'supercomputing', 'journal', 'difficult', 'became', 'article', 'supercomputers', 'their', 'j', 'importance', 'is', 'covering', 'improvements', 'rosillo', 'are', 'written', 'take', 'along', 'historical'}\n"
     ]
    }
   ],
   "source": [
    "#Unique words \n",
    "vocabulary=set(tokens)\n",
    "print(len(vocabulary))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ffd53",
   "metadata": {},
   "source": [
    "### n-grams model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16deaa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens=tokenizer(paragraph)\n",
    "\n",
    "# Create a vocabulary from text tokens\n",
    "\n",
    "# tokenize the 'paragraph' text using the provided tokenizer.\n",
    "# The map function applies the tokenizer to each word in the 'paragraph' after splitting it.\n",
    "# The result is a list of tokens representing the words in the 'paragraph'.\n",
    "tokenized_paragraph = map(tokenizer, paragraph.split())\n",
    "\n",
    "# Step 2: Vocabulary Building\n",
    "# The build_vocab_from_iterator function constructs a vocabulary from the tokenized text.\n",
    "# In this case, add a special token \"<unk>\" (unknown token) to handle out-of-vocabulary words.\n",
    "vocab = build_vocab_from_iterator(tokenized_paragraph, specials=[\"<unk>\"])\n",
    "\n",
    "# Step 3: Set Default Index\n",
    "# Set the default index for the vocabulary to the index corresponding to the \"<unk>\" token.\n",
    "# This ensures that any unknown tokens in the future will be mapped to this index.\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e16c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9d40234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'the', '.', ',', 'and', 'of', 'in', 'networks', 'supercomputers', 'article', 'authors', 'challenges', 'have', 'supercomputing', 'covered', 'future', 'history', 'how', 'improvements', 'on', 'their', 'they', 'written', 'á', '2015', 'along', 'also', 'an', 'are', 'became', 'been', 'birth', 'by', 'characteristics', 'communication”', 'covering', 'current', 'detail', 'developing', 'difficult', 'efficient', 'energy', 'fernández-gonzález', 'functions', 'historical', 'importance', 'important', 'is', 'it', 'j', 'journal', 'major', 'matellán', 'mention', 'miguel-dávila', 'more', 'not', 'november', 'plots', 'published', 'r', 'revealing', 'review', 'reviews', 'rosillo', 'scientific', 'support', 'take', 'task', 'that', 'timely', 'unfortunately', 'use', 'v', 'was', 'with', '“historical']\n"
     ]
    }
   ],
   "source": [
    "index_to_token = vocab.get_itos()\n",
    "print(index_to_token) #index of each words. search for a word from index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3698e1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['article', 'the'], 'is'), (['is', 'article'], '“historical'), (['“historical', 'is'], 'review'), (['review', '“historical'], 'and'), (['and', 'review'], 'future'), (['future', 'and'], 'challenges'), (['challenges', 'future'], 'in'), (['in', 'challenges'], 'supercomputing'), (['supercomputing', 'in'], 'and'), (['and', 'supercomputing'], 'networks'), (['networks', 'and'], 'of'), (['of', 'networks'], 'scientific'), (['scientific', 'of'], 'communication”'), (['communication”', 'scientific'], ','), ([',', 'communication”'], 'written'), (['written', ','], 'by'), (['by', 'written'], 'á'), (['á', 'by'], '.'), (['.', 'á'], 'fernández-gonzález'), (['fernández-gonzález', '.'], ','), ([',', 'fernández-gonzález'], 'r'), (['r', ','], '.'), (['.', 'r'], 'rosillo'), (['rosillo', '.'], ','), ([',', 'rosillo'], 'j'), (['j', ','], '.'), (['.', 'j'], 'á'), (['á', '.'], '.'), (['.', 'á'], 'miguel-dávila'), (['miguel-dávila', '.'], ','), ([',', 'miguel-dávila'], 'and'), (['and', ','], 'v'), (['v', 'and'], '.'), (['.', 'v'], 'matellán'), (['matellán', '.'], '.'), (['.', 'matellán'], 'it'), (['it', '.'], 'was'), (['was', 'it'], 'published'), (['published', 'was'], 'in'), (['in', 'published'], 'the'), (['the', 'in'], 'journal'), (['journal', 'the'], 'of'), (['of', 'journal'], 'supercomputing'), (['supercomputing', 'of'], 'in'), (['in', 'supercomputing'], 'november'), (['november', 'in'], '2015'), (['2015', 'november'], '.'), (['.', '2015'], 'the'), (['the', '.'], 'article'), (['article', 'the'], 'reviews'), (['reviews', 'article'], 'the'), (['the', 'reviews'], 'history'), (['history', 'the'], 'of'), (['of', 'history'], 'supercomputers'), (['supercomputers', 'of'], 'covering'), (['covering', 'supercomputers'], 'their'), (['their', 'covering'], 'birth'), (['birth', 'their'], ','), ([',', 'birth'], 'current'), (['current', ','], 'use'), (['use', 'current'], ','), ([',', 'use'], 'and'), (['and', ','], 'future'), (['future', 'and'], 'challenges'), (['challenges', 'future'], '.'), (['.', 'challenges'], 'the'), (['the', '.'], 'characteristics'), (['characteristics', 'the'], ','), ([',', 'characteristics'], 'improvements'), (['improvements', ','], ','), ([',', 'improvements'], 'and'), (['and', ','], 'challenges'), (['challenges', 'and'], 'of'), (['of', 'challenges'], 'major'), (['major', 'of'], 'supercomputers'), (['supercomputers', 'major'], 'are'), (['are', 'supercomputers'], 'covered'), (['covered', 'are'], '.'), (['.', 'covered'], 'the'), (['the', '.'], 'authors'), (['authors', 'the'], 'also'), (['also', 'authors'], 'mention'), (['mention', 'also'], 'the'), (['the', 'mention'], 'importance'), (['importance', 'the'], 'of'), (['of', 'importance'], 'networks'), (['networks', 'of'], 'that'), (['that', 'networks'], 'support'), (['support', 'that'], 'the'), (['the', 'support'], 'functions'), (['functions', 'the'], 'of'), (['of', 'functions'], 'supercomputers'), (['supercomputers', 'of'], ','), ([',', 'supercomputers'], 'along'), (['along', ','], 'with'), (['with', 'along'], 'the'), (['the', 'with'], 'history'), (['history', 'the'], 'of'), (['of', 'history'], 'how'), (['how', 'of'], 'the'), (['the', 'how'], 'networks'), (['networks', 'the'], 'have'), (['have', 'networks'], 'been'), (['been', 'have'], 'developing'), (['developing', 'been'], '.'), (['.', 'developing'], 'the'), (['the', '.'], 'authors'), (['authors', 'the'], 'have'), (['have', 'authors'], 'written'), (['written', 'have'], 'an'), (['an', 'written'], 'important'), (['important', 'an'], 'and'), (['and', 'important'], 'timely'), (['timely', 'and'], 'article'), (['article', 'timely'], 'on'), (['on', 'article'], 'supercomputing'), (['supercomputing', 'on'], 'and'), (['and', 'supercomputing'], 'their'), (['their', 'and'], 'networks'), (['networks', 'their'], 'and'), (['and', 'networks'], 'they'), (['they', 'and'], 'take'), (['take', 'they'], 'on'), (['on', 'take'], 'the'), (['the', 'on'], 'difficult'), (['difficult', 'the'], 'task'), (['task', 'difficult'], 'of'), (['of', 'task'], 'revealing'), (['revealing', 'of'], 'the'), (['the', 'revealing'], 'historical'), (['historical', 'the'], 'plots'), (['plots', 'historical'], 'and'), (['and', 'plots'], 'improvements'), (['improvements', 'and'], 'of'), (['of', 'improvements'], 'supercomputers'), (['supercomputers', 'of'], '.'), (['.', 'supercomputers'], 'unfortunately'), (['unfortunately', '.'], ','), ([',', 'unfortunately'], 'the'), (['the', ','], 'authors'), (['authors', 'the'], 'have'), (['have', 'authors'], 'not'), (['not', 'have'], 'covered'), (['covered', 'not'], 'how'), (['how', 'covered'], 'they'), (['they', 'how'], 'became'), (['became', 'they'], 'more'), (['more', 'became'], 'energy'), (['energy', 'more'], 'efficient'), (['efficient', 'energy'], 'in'), (['in', 'efficient'], 'detail'), (['detail', 'in'], '.')]\n",
      "----- Index the text -----\n",
      "context ['article', 'the'] target is\n",
      "context index [9, 1] target index [47]\n",
      "----- Embed the text -----\n",
      "[9, 1]\n",
      "tensor([[ 0.2565, -1.0004,  0.5346, -2.9850, -0.6267,  0.9594, -0.3538,  0.0954,\n",
      "          1.3225,  1.3100, -0.0155,  1.0599,  0.9551, -1.8163, -1.1299, -0.1099,\n",
      "         -0.1089,  0.4690,  0.4959, -0.2715],\n",
      "        [-0.8134,  0.0872, -0.3121,  0.3779,  0.2326, -0.1501, -2.6114, -0.9592,\n",
      "          0.0742, -0.4496,  1.9023,  1.4638, -0.4168,  0.6506,  0.1875, -1.4936,\n",
      "         -0.3640, -1.5028, -0.0144,  2.0957]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([2, 20])\n",
      "----- concatenate the embedded text -----\n",
      "tensor([[ 0.2565, -1.0004,  0.5346, -2.9850, -0.6267,  0.9594, -0.3538,  0.0954,\n",
      "          1.3225,  1.3100, -0.0155,  1.0599,  0.9551, -1.8163, -1.1299, -0.1099,\n",
      "         -0.1089,  0.4690,  0.4959, -0.2715, -0.8134,  0.0872, -0.3121,  0.3779,\n",
      "          0.2326, -0.1501, -2.6114, -0.9592,  0.0742, -0.4496,  1.9023,  1.4638,\n",
      "         -0.4168,  0.6506,  0.1875, -1.4936, -0.3640, -1.5028, -0.0144,  2.0957]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "----- input to linear function and get output -----\n",
      "tensor([[-2.6901e-01, -9.4067e-01, -2.5907e-01,  2.2621e-01, -2.5570e-01,\n",
      "         -1.9763e-01, -2.3873e-01, -4.5498e-01,  1.2182e+00, -4.5499e-01,\n",
      "          7.2461e-01, -2.4268e-01, -7.5013e-01,  1.8402e-01,  4.9081e-01,\n",
      "          4.2492e-01, -1.8797e-01,  1.1088e+00,  1.9089e-01,  1.8313e-01,\n",
      "         -9.2146e-03, -4.5853e-01,  1.9068e-01,  8.2090e-01, -4.6508e-02,\n",
      "         -1.2951e+00, -8.6037e-02, -3.0171e-01, -5.1105e-01,  1.6461e+00,\n",
      "          4.9970e-01,  8.2182e-01, -1.9889e-01, -2.9995e-02, -1.0204e+00,\n",
      "          4.8735e-01, -3.5972e-01,  1.1737e+00, -8.4340e-02,  1.0703e+00,\n",
      "         -7.1476e-01, -9.7373e-02,  6.3344e-01, -8.3465e-01,  6.4344e-01,\n",
      "          8.5463e-01, -1.2326e-03, -8.7353e-01, -1.1907e+00,  5.1414e-01,\n",
      "         -6.2052e-02,  8.1290e-01, -5.5513e-02,  1.9537e-01,  1.3942e-01,\n",
      "         -8.9927e-01, -8.4567e-01, -5.9315e-01, -4.7897e-01,  9.9695e-03,\n",
      "          2.9520e-01, -2.7006e-01, -4.9094e-01, -7.6776e-01, -3.8537e-01,\n",
      "          7.3779e-01,  8.6217e-01,  1.4614e-01,  1.3080e-02, -5.8586e-01,\n",
      "         -8.3780e-03, -4.6104e-01,  7.9097e-01,  8.5481e-01,  4.3965e-01,\n",
      "         -1.9323e-01, -1.1433e-01, -5.5385e-01, -5.3120e-01, -3.8332e-02,\n",
      "          2.7901e-01, -3.0693e-01, -1.5153e-02,  4.6058e-01, -5.2360e-01,\n",
      "          4.9053e-02,  1.5900e-01, -8.7366e-02, -8.4129e-01, -3.6872e-01,\n",
      "         -5.8437e-02,  3.6911e-01, -2.0167e-01, -2.6156e-01,  5.2970e-01,\n",
      "          1.9317e-01,  1.9827e-01, -1.5864e-01, -1.3042e-01,  3.0231e-01,\n",
      "         -2.7279e-01, -3.2338e-01, -7.1281e-02, -7.5457e-01,  1.9324e-01,\n",
      "          1.1923e-01, -8.7472e-01,  8.5097e-02,  9.7351e-02, -4.9801e-01,\n",
      "         -8.0556e-01, -4.6842e-01,  5.3538e-02,  5.3466e-01, -9.0555e-01,\n",
      "         -9.3486e-02, -1.5368e-01, -2.2464e-01, -4.6177e-01, -3.6293e-01,\n",
      "          7.0248e-01,  7.3764e-01,  4.2720e-01, -1.4504e+00, -2.1795e-01,\n",
      "         -6.8774e-01, -1.2240e+00, -3.7894e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Example use\n",
    "\n",
    "#get n-grams\n",
    "CONTEXT_SIZE=2\n",
    "ngrams = [\n",
    "    (\n",
    "        [tokens[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        tokens[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(tokens))\n",
    "]\n",
    "\n",
    "print(ngrams)\n",
    "\n",
    "#Embedding\n",
    "embedding_dim=20\n",
    "vocab_size=len(vocab)\n",
    "embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "print(\"-----\", \"Index the text\", \"-----\")\n",
    "context, target=ngrams[0]\n",
    "print(\"context\",context,\"target\",target)\n",
    "print(\"context index\",vocab(context),\"target index\",vocab([target]))\n",
    "\n",
    "print(\"-----\", \"Embed the text\", \"-----\")\n",
    "print(vocab(context))\n",
    "my_embeddings=embeddings(torch.tensor(vocab(context)))\n",
    "print(my_embeddings)\n",
    "print(my_embeddings.shape)\n",
    "\n",
    "print(\"-----\", \"concatenate the embedded text\", \"-----\")\n",
    "my_embeddings=my_embeddings.reshape(1,-1)\n",
    "my_embeddings.shape\n",
    "print(my_embeddings)\n",
    "\n",
    "\n",
    "linear = nn.Linear(embedding_dim*CONTEXT_SIZE,128)\n",
    "print(\"-----\", \"input to linear function and get output\", \"-----\")\n",
    "print(linear(my_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b35dc764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batching\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CONTEXT_SIZE=3\n",
    "BATCH_SIZE=10\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "def collate_batch(batch):\n",
    "    batch_size=len(batch)\n",
    "    context, target=[],[]\n",
    "    for i in range(CONTEXT_SIZE, batch_size):\n",
    "        target.append(vocab([batch[i]]))\n",
    "        context.append(vocab([batch[i-j-1] for j in range(CONTEXT_SIZE)]))\n",
    "\n",
    "    return   torch.tensor(context).to(device),  torch.tensor(target).to(device).reshape(-1)\n",
    "\n",
    "Padding=BATCH_SIZE-len(tokens)%BATCH_SIZE\n",
    "tokens_pad=tokens+tokens[0:Padding]  #add first few tokens to the end for padding.\n",
    "\n",
    "dataloader = DataLoader(tokens_pad, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bddb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the our n-gram\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.context_size=context_size\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds=torch.reshape( embeds, (-1,self.context_size * self.embedding_dim))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ddb3399-29da-484a-9553-30c37f203af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This function writes a paragraph to use during the training process.\n",
    "def write_paragraph(model,number_of_words=100):\n",
    "    my_paragraph=\"\"\n",
    "    for i in range(number_of_words):\n",
    "        with torch.no_grad():\n",
    "            context=torch.tensor(vocab([tokens[i-j-1] for j in range(CONTEXT_SIZE)])).to(device)\n",
    "            word_inx=torch.argmax(model(context))\n",
    "            my_paragraph+=\" \"+index_to_token[word_inx.detach().item()]\n",
    "\n",
    "    return my_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68456a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function that initiates back propagation\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8b200c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function trains the n-gram model expressed as a neural network\n",
    "\n",
    "def train(dataloader, model, number_of_epochs=100, show=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader containing training data.\n",
    "        model (nn.Module): Neural network model to be trained.\n",
    "        number_of_epochs (int, optional): Number of epochs for training. Default is 100.\n",
    "        show (int, optional): Interval for displaying progress. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing loss values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    MY_LOSS = []  # List to store loss values for each epoch\n",
    "\n",
    "    # Iterate over the specified number of epochs\n",
    "    for epoch in tqdm(range(number_of_epochs)):\n",
    "        total_loss = 0  # Initialize total loss for the current epoch\n",
    "        my_paragraph = \"\"    # Initialize a string to store the generated song\n",
    "\n",
    "        # Iterate over batches in the dataloader\n",
    "        for context, target in dataloader:\n",
    "            model.zero_grad()          # Zero the gradients to avoid accumulation\n",
    "            predicted = model(context)  # Forward pass through the model to get predictions\n",
    "            loss = criterion(predicted, target.reshape(-1))  # Calculate the loss\n",
    "            total_loss += loss.item()   # Accumulate the loss\n",
    "\n",
    "            loss.backward()    # Backpropagation to compute gradients\n",
    "            optimizer.step()   # Update model parameters using the optimizer\n",
    "\n",
    "        # Display progress and generate song at specified intervals\n",
    "        if epoch % show == 0:\n",
    "            my_paragraph += write_paragraph(model)  # Generate song using the model\n",
    "\n",
    "            print(\"Generated Paragraph:\")\n",
    "            print(\"\\n\")\n",
    "            print(my_paragraph)\n",
    "\n",
    "        MY_LOSS.append(total_loss/len(dataloader))  # Append the total loss for the epoch to MY_LOSS list\n",
    "\n",
    "    return MY_LOSS  # Return the list of  mean loss values for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c55d93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input the preferred context size: 3\n"
     ]
    }
   ],
   "source": [
    "# Define the context size for the n-gram model\n",
    "CONTEXT_SIZE = int(input(\"Input the preferred context size:\"))\n",
    "\n",
    "# Create an instance of the NGramLanguageModeler class with specified parameters\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "\n",
    "# Define the optimizer for training the model, using stochastic gradient descent (SGD)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Set up a learning rate scheduler using StepLR to adjust the learning rate during training\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "789690ed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:07, 13.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph:\n",
      "\n",
      "\n",
      " been difficult and of <unk> and that covering have of been of of more <unk> major energy and in been review difficult of review difficult difficult have the have they and supercomputing and functions covering review difficult and and of and have future of and reviews the of task the in and difficult have . of <unk> have take and . major have covering major major have are difficult of it covering and history covering of <unk> have in . the important of difficult in communication” in of <unk> the have á of in <unk> have history have major review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:01<00:06, 13.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph:\n",
      "\n",
      "\n",
      " . . and . . and . of . of . . of . . the . and . . . . . . . . . . . . . . . . . . . and . . . . . . . . the . . the . and . . . of . the . . . . . . . of . the . . . . . . . of . the . . the . . . . . . . . the . . of . . the . . of .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:01<00:05, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph:\n",
      "\n",
      "\n",
      " the . and . . and . of . of . . of supercomputers . the . . . . . . of . the . . . . . . . of . of . . and . . . . . of and . the . . the . and . . . of . the . , . . . . . of . the . of . . and . of of . the . . the . of . . . of . . the . . of supercomputers . the . . of of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:02<00:05, 12.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph:\n",
      "\n",
      "\n",
      " the . and . . and , challenges . the of . of supercomputers . , and and . . the . of . the . and . and . the . and . of supercomputers . and . the and , of of and the the , . the . and . , of supercomputers . the . , . supercomputers , and future challenges . the . , . . and . of supercomputers . the the . the . of . . . of supercomputers supercomputers the , of of supercomputers . the . . of of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:03<00:04, 13.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph:\n",
      "\n",
      "\n",
      " the . and . review and , challenges . the of . challenges supercomputers and , and , , . the . the . the . and . and . the . and future of supercomputers . and . the and , of of and and the , . the . and . , of supercomputers . the of , . supercomputers , and future challenges . the . , . . and . of supercomputers . the the . the , of . the of of supercomputers supercomputers the , of of supercomputers . and . . of of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:03<00:03, 14.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph:\n",
      "\n",
      "\n",
      " the . and “historical review and future challenges . the of future challenges supercomputers communication” , and by á . the . the . the , and . and . the . and future of supercomputers . the . , and , of of and in november , . the . and . , of supercomputers . the of , . supercomputers , and future challenges . the . , . . and . of major . the the . the , have . the of of supercomputers supercomputers the , of of supercomputers . and supercomputers . of of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:04<00:03, 12.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph:\n",
      "\n",
      "\n",
      " the . and “historical review and future challenges . the of future challenges supercomputers communication” , and by á . the . the . the , and . and . the . and v of supercomputers . the . published and , of of supercomputing in november 2015 . the . and . history of supercomputers . their of , . supercomputers , and future challenges . the . , . . and challenges of major . the the . the , have . the of of supercomputers that the , of of supercomputers . and challenges . of of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [00:05<00:02, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph:\n",
      "\n",
      "\n",
      " the article have “historical review and future challenges . supercomputing of future challenges supercomputers communication” , written by á . the . have . the , and . á . the . and v of matellán . the was published and , of of supercomputing in november 2015 . the . have . history of supercomputers . their birth , and supercomputers , and future challenges . the . , . . á challenges of major supercomputers the the . the , have . the importance of supercomputers that the , of of supercomputers . and challenges the of of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [00:06<00:01, 12.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph:\n",
      "\n",
      "\n",
      " the article is “historical review and future challenges in supercomputing of future challenges supercomputers communication” , written by á . the . have . rosillo , j . á . the . and v of matellán . the was published and november of of supercomputing in november 2015 . the . is . history of supercomputers . their birth , and supercomputers , and future challenges in the . , . . á challenges of major supercomputers their covered . the , have . the importance of supercomputers that the , of of supercomputers , and challenges the of of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [00:07<00:00, 12.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph:\n",
      "\n",
      "\n",
      " the article is “historical review and future challenges in supercomputing of future challenges scientific communication” , written by á . the . j . rosillo , j . á . the . and v of matellán . it was published and november of of supercomputing in november 2015 . the history is . history of supercomputers . their birth , and supercomputers , and future challenges in the . , . of á challenges of major supercomputers their covered . the , have mention the importance of supercomputers that the history of of supercomputers , along with the history of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#Train!\n",
    "y_loss=train(dataloader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aec73699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Save the trained result\n",
    "save_path = str(CONTEXT_SIZE) + '_gram.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "#my_loss_list.append(my_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6d6eff2-d5d2-4a52-ac38-0f54b94c2e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This function writes a paragraph starting with a word of your choice.\n",
    "def write_paragraph_with_input(model,input_word, number_of_words):\n",
    "    my_paragraph=[\"<unk>\",\"<unk>\",\"<unk>\",input_word]\n",
    "    for i in range(number_of_words):\n",
    "        with torch.no_grad():\n",
    "            context=torch.tensor(vocab([tokens[i-j-1] for j in range(CONTEXT_SIZE)])).to(device)\n",
    "            word_inx=torch.argmax(model(context))\n",
    "            my_paragraph.append(index_to_token[word_inx.detach().item()])\n",
    "\n",
    "    return my_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55988b0f-ca09-4638-9c87-1bc41b7eeef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What word do you want to being the paragraph with? author\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author the article is “historical review and future challenges in supercomputing of future challenges scientific communication” , written by á . the . j . rosillo , j . á . it , and v of matellán . it was published and november of of supercomputing in november 2015 . the history is . history of supercomputers . their birth , and supercomputers , and future challenges in the . , . of á challenges of major supercomputers their covered . the , have mention the importance of supercomputers that support history of of supercomputers , along with the history of \n"
     ]
    }
   ],
   "source": [
    "first_word = input(\"What word do you want to being the paragraph with?\")\n",
    "paragraph_output = write_paragraph_with_input(model, first_word, 100)\n",
    "\n",
    "paragraph_output_str = \"\"\n",
    "for i in paragraph_output[3:]:\n",
    "    paragraph_output_str += i + \" \"\n",
    "print(paragraph_output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c480b15-8e81-4d19-9d06-3a147035c7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
